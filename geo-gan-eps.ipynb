{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fastai \n",
    "# from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import imshow, imsave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "df = pd.read_csv('./inputs/dummy-class/t001.eps', sep=' ',\n",
    "                 header=None, \n",
    "                 dtype={'0': 'int', '5': 'string'},\n",
    "                 skiprows=21,\n",
    "                 nrows=5000)\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "print(df[[0]].min())\n",
    "print(df[[1]].min())\n",
    "print(df[[3]].min())\n",
    "print(df[[0]].max())\n",
    "print(df[[1]].max())\n",
    "print(df[[3]].max())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_max = 590\n",
    "eps_rows = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought I would need to save the bounding box and other header info, but I can just calculate that bounding box dynamically based on mins and maxs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "F = open('workfile,eps','w')\n",
    "F.write('%!PS-Adobe-3.0 EPSF-3.0\\n')\n",
    "F.write('%%BoundingBox: -90 -32 553 333\\n\\n')\n",
    "F.write('/bd { bind def } bind def\\n')\n",
    "F.write('/co { setrgbcolor } bd\\n')\n",
    "F.write('/lw { setlinewidth } bd\\n')\n",
    "F.write('/st { stroke } bd\\n')\n",
    "F.write('/fi { fill } bd\\n')\n",
    "F.write('/cp { closepath } bd\\n')\n",
    "F.write('/mo { newpath moveto } bd\\n')\n",
    "F.write('/to { lineto } bd\\n')\n",
    "F.write('/li { mo to st } bd\\n')\n",
    "F.write('/ci { newpath 0 360 arc } bd\\n')\n",
    "F.write('gsave\\n')\n",
    "F.write('1 lw\\n')\n",
    "F.write('2 setlinecap\\n')\n",
    "F.write('0 setlinejoin\\n')\n",
    "F.write('0.078431 0.588235 0.823529 co\\n\\n')\n",
    "F.writelines(df.to_string(header=False, index=False)) \n",
    "F.close() \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'VanillaGAN'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce GTX 1050\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "print('Using device:', DEVICE)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if hasattr(DEVICE, 'type') and DEVICE.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_cols=4\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_image(y_hat):\n",
    "    result = y_hat.mul(eps_max).cpu().int().data.numpy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could not broadcast input array from shape (500,5000) into shape (500,2500)\n",
    "# could not broadcast input array from shape (250,1250) into shape (250,2500)\n",
    "# could not broadcast input array from shape (250,1250) into shape (250,2500)\n",
    "\n",
    "def get_sample_image(G, n_noise):\n",
    "    \"\"\"\n",
    "        generate 5 images\n",
    "    \"\"\"\n",
    "    num_images = 5\n",
    "    z = torch.randn(num_images, n_noise).to(DEVICE)\n",
    "    # print(z[0][0:100])\n",
    "    y_hat = G(z).view(num_images, eps_rows, image_cols)\n",
    "    result = y_hat.mul(eps_max).cpu().int().data.numpy()\n",
    "    # result = result.view(25, image_rows, image_cols)\n",
    "    # print(result[0])\n",
    "    # print(result[0].shape)\n",
    "    return result\n",
    "\n",
    "    img = np.zeros([image_size * 5, image_size * 5])\n",
    "    for j in range(5):\n",
    "        # b = np.concatenate([np.where(x > 0.5, 1, 0) for x in result[j*5:(j+1)*5]], axis=-1)\n",
    "        b = np.concatenate([x for x in result[j*5:(j+1)*5]], axis=-1)\n",
    "        img[j*image_size:(j+1)*image_size] = b\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[[-22   6  24 -22]\n",
    "  [ 23  -6  47  19]\n",
    "  [-67  29  -5  -6]\n",
    "  ...\n",
    "  [-13 -21 -26 -15]\n",
    "  [-19 -26 -51  26]\n",
    "  [-35 -22  19   7]]\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "img = get_sample_image(G, n_noise)\n",
    "# print(img.shape)\n",
    "# print(img)\n",
    "img_lines = img[0]\n",
    "print(img_lines.shape)\n",
    "\n",
    "\n",
    "for j in range(image_rows):\n",
    "    # print(img_lines[j])\n",
    "    print('{} {} {} {} li\\n'.format(img_lines[j][0], img_lines[j][1], img_lines[j][2], img_lines[j][3]))\n",
    "    \n",
    "    \n",
    "# img_lines_str = ' '.join(str(y) + ' li\\n' for y in x for x in img_lines)\n",
    "print(img_lines_str)\n",
    "# a_str = ' '.join(str(y) + ' li\\n' for y in x for x in img[0])\n",
    "\n",
    "# print(a_str)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom left width. Make this number smaller (more negative) to increase the starting offset from the left\n",
    "bound_w_min = -320\n",
    "# Bottom left height. Make this number smaller to increase the height of the image.\n",
    "bound_h_min = 420\n",
    "# Top left width. Make this number larger to increase the width of the image.\n",
    "bound_w_max = 100\n",
    "# Top left height. Make this number smaller to increase the offset from the top of the image.\n",
    "bound_h_max = 590"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_save(path, img):\n",
    "    # img = get_sample_image(G, n_noise)\n",
    "    # print(path)\n",
    "    F = open(path,'w')\n",
    "    F.write('%!PS-Adobe-3.0 EPSF-3.0\\n')\n",
    "    F.write('%%BoundingBox: {} {} {} {}\\n\\n\\n\\n'.format(bound_w_min, bound_h_min, bound_w_max, bound_h_max,))\n",
    "    F.write('/bd { bind def } bind def\\n')\n",
    "    F.write('/co { setrgbcolor } bd\\n')\n",
    "    F.write('/lw { setlinewidth } bd\\n')\n",
    "    F.write('/st { stroke } bd\\n')\n",
    "    F.write('/fi { fill } bd\\n')\n",
    "    F.write('/cp { closepath } bd\\n')\n",
    "    F.write('/mo { newpath moveto } bd\\n')\n",
    "    F.write('/to { lineto } bd\\n')\n",
    "    F.write('/li { mo to st } bd\\n')\n",
    "    F.write('/ci { newpath 0 360 arc } bd\\n')\n",
    "    F.write('gsave\\n')\n",
    "    F.write('1 lw\\n')\n",
    "    F.write('2 setlinecap\\n')\n",
    "    F.write('0 setlinejoin\\n')\n",
    "    F.write('0 0 0 co\\n')\n",
    "    \n",
    "    for j in range(eps_rows):\n",
    "         # print(img[j][0])\n",
    "         F.write('{} {} {} {} li\\n'.format(img[j][0], img[j][1], img[j][2], img[j][3]))\n",
    "    \n",
    "    F.write('grestore\\n')\n",
    "    F.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_save_series(step, images):\n",
    "    for i in range(len(images)): \n",
    "        eps_save('./samples/{}_step{:07d}_{}.eps'.format(MODEL_NAME, step, i), images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple Discriminator w/ MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=eps_rows * image_cols, num_classes=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            # nn.Linear(input_size, 256),            \n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            # nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, num_classes),\n",
    "            # nn.Linear(128, num_classes),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_ = x.view(x.size(0), -1)\n",
    "        y_ = self.layer(y_)\n",
    "        return y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple Generator w/ MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=batch_size, num_classes=eps_rows * image_cols):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(num_classes, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            # nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            # nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, num_classes),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_ = self.layer(x)\n",
    "        y_ = y_.view(x.size(0), 1, eps_rows, image_cols)\n",
    "        return y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_noise = eps_rows * image_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_one_eps(item_path):\n",
    "    # print(item_path)\n",
    "    df = pd.read_csv(item_path, sep=' ',\n",
    "                 header=None, \n",
    "                 # names=['x1', 'y1', 'x2', 'y2', 'cmd'],\n",
    "                 dtype={0: 'int64', 1: 'int64', 2: 'int64', 3: 'int64', 4: 'string'},\n",
    "                 skiprows=20,\n",
    "                 nrows=eps_rows)\n",
    "    # print(df.loc[:,0:3])\n",
    "    \n",
    "    # TODO: Make this into a transform?\n",
    "    tensor = torch.from_numpy(df.loc[:,0:3].values).float().div(eps_max)\n",
    "    # img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n",
    "    # tensor = torch.from_numpy(np.array(df.loc[:,0:3].values, np.uint8, copy=False)).float().div(338)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset():\n",
    "    data_path = 'inputs'\n",
    "    train_dataset = torchvision.datasets.DatasetFolder(\n",
    "        root=data_path,\n",
    "        loader=load_one_eps,\n",
    "        extensions='eps',\n",
    "        transform=transforms.Compose([\n",
    "        #    torchvision.transforms.ToTensor(),\n",
    "        #    torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "        #    torchvision.transforms.RandomHorizontalFlip(),\n",
    "        #    torchvision.transforms.RandomVerticalFlip(),\n",
    "        #    torchvision.transforms.RandomResizedCrop(248, scale=(0.8, 1.), ratio=(1., 1.)),\n",
    "        #    # tochvision.transforms.RandomAffine(0, scale=(1.,1.8), fillcolor=255, shear=None),\n",
    "        #    torchvision.transforms.ToTensor(),\n",
    "         #   transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: (5, 500, 4)\n"
     ]
    }
   ],
   "source": [
    "# Show a batch of sample images.\n",
    "imgs, _ = next(iter(load_dataset()))\n",
    "print('Batch shape:',imgs.numpy().shape)\n",
    "# print(imgs.min())\n",
    "# print(imgs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 100000 # need more than 10 epochs for training generator\n",
    "n_critic = 1 # for training more k steps about Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_labels = torch.ones(batch_size, 1).to(DEVICE) # Discriminator Label to real\n",
    "D_fakes = torch.zeros(batch_size, 1).to(DEVICE) # Discriminator Label to fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('samples'):\n",
    "    os.makedirs('samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, file_name='checkpoint.pth.tar'):\n",
    "    torch.save(state, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "D = Discriminator().to(DEVICE)\n",
    "G = Generator(n_noise).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.BCELoss()\n",
    "#D_opt = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "#G_opt = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "D_opt = torch.optim.AdamW(D.parameters(), lr=0.0001, betas=(0.4, 0.999))\n",
    "G_opt = torch.optim.AdamW(G.parameters(), lr=0.0001, betas=(0.4, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100000, Step: 1600000, D Loss: 1.1684578657150269, G Loss: 1.423330545425415\n",
      "Epoch: 500/100000, Step: 1601000, D Loss: 0.9472920894622803, G Loss: 1.2158310413360596\n",
      "Epoch: 1000/100000, Step: 1602000, D Loss: 1.083784818649292, G Loss: 1.1252762079238892\n",
      "Epoch: 1500/100000, Step: 1603000, D Loss: 0.6921959519386292, G Loss: 0.89454585313797\n",
      "Epoch: 2000/100000, Step: 1604000, D Loss: 0.7355842590332031, G Loss: 1.1757228374481201\n",
      "Epoch: 2500/100000, Step: 1605000, D Loss: 0.8673501014709473, G Loss: 1.217210054397583\n",
      "Epoch: 3000/100000, Step: 1606000, D Loss: 0.9523478746414185, G Loss: 1.2137908935546875\n",
      "Epoch: 3500/100000, Step: 1607000, D Loss: 0.8824858665466309, G Loss: 1.2675198316574097\n",
      "Epoch: 4000/100000, Step: 1608000, D Loss: 0.9947152137756348, G Loss: 1.1962215900421143\n",
      "Epoch: 4500/100000, Step: 1609000, D Loss: 0.8997101783752441, G Loss: 1.1791092157363892\n",
      "Epoch: 5000/100000, Step: 1610000, D Loss: 1.0954169034957886, G Loss: 1.2841975688934326\n",
      "Epoch: 5500/100000, Step: 1611000, D Loss: 0.5440990924835205, G Loss: 1.4490344524383545\n",
      "Epoch: 6000/100000, Step: 1612000, D Loss: 0.8371177911758423, G Loss: 1.1981006860733032\n",
      "Epoch: 6500/100000, Step: 1613000, D Loss: 0.9845967292785645, G Loss: 1.4485241174697876\n",
      "Epoch: 7000/100000, Step: 1614000, D Loss: 0.8257410526275635, G Loss: 1.304620385169983\n",
      "Epoch: 7500/100000, Step: 1615000, D Loss: 1.2595382928848267, G Loss: 1.4780046939849854\n",
      "Epoch: 8000/100000, Step: 1616000, D Loss: 1.3038125038146973, G Loss: 1.0125892162322998\n",
      "Epoch: 8500/100000, Step: 1617000, D Loss: 1.0278875827789307, G Loss: 1.0526130199432373\n",
      "Epoch: 9000/100000, Step: 1618000, D Loss: 0.957992672920227, G Loss: 0.9893094897270203\n",
      "Epoch: 9500/100000, Step: 1619000, D Loss: 0.8390108346939087, G Loss: 1.2432878017425537\n",
      "Epoch: 10000/100000, Step: 1620000, D Loss: 1.025377631187439, G Loss: 1.098329782485962\n",
      "Epoch: 10500/100000, Step: 1621000, D Loss: 0.7327404022216797, G Loss: 1.0470991134643555\n",
      "Epoch: 11000/100000, Step: 1622000, D Loss: 0.8262600898742676, G Loss: 1.461069941520691\n",
      "Epoch: 11500/100000, Step: 1623000, D Loss: 1.0087766647338867, G Loss: 1.9073413610458374\n",
      "Epoch: 12000/100000, Step: 1624000, D Loss: 0.7394552230834961, G Loss: 1.333252191543579\n",
      "Epoch: 12500/100000, Step: 1625000, D Loss: 0.6934525966644287, G Loss: 1.2496402263641357\n",
      "Epoch: 13000/100000, Step: 1626000, D Loss: 0.9584541320800781, G Loss: 0.9726899266242981\n",
      "Epoch: 13500/100000, Step: 1627000, D Loss: 0.9360395073890686, G Loss: 1.1228737831115723\n",
      "Epoch: 14000/100000, Step: 1628000, D Loss: 1.195102334022522, G Loss: 1.1662626266479492\n",
      "Epoch: 14500/100000, Step: 1629000, D Loss: 0.8484392762184143, G Loss: 1.1398210525512695\n",
      "Epoch: 15000/100000, Step: 1630000, D Loss: 1.041579008102417, G Loss: 1.153397798538208\n",
      "Epoch: 15500/100000, Step: 1631000, D Loss: 0.9486490488052368, G Loss: 1.1646589040756226\n",
      "Epoch: 16000/100000, Step: 1632000, D Loss: 1.2373039722442627, G Loss: 1.2527828216552734\n",
      "Epoch: 16500/100000, Step: 1633000, D Loss: 1.1795917749404907, G Loss: 1.014552116394043\n",
      "Epoch: 17000/100000, Step: 1634000, D Loss: 1.0134093761444092, G Loss: 1.1524138450622559\n",
      "Epoch: 17500/100000, Step: 1635000, D Loss: 1.0158650875091553, G Loss: 1.2448617219924927\n",
      "Epoch: 18000/100000, Step: 1636000, D Loss: 1.0852173566818237, G Loss: 1.0318968296051025\n",
      "Epoch: 18500/100000, Step: 1637000, D Loss: 0.6257691383361816, G Loss: 1.3301401138305664\n",
      "Epoch: 19000/100000, Step: 1638000, D Loss: 0.7022690176963806, G Loss: 1.1246341466903687\n",
      "Epoch: 19500/100000, Step: 1639000, D Loss: 1.2749282121658325, G Loss: 1.1393693685531616\n",
      "Epoch: 20000/100000, Step: 1640000, D Loss: 1.1732523441314697, G Loss: 1.055831789970398\n",
      "Epoch: 20500/100000, Step: 1641000, D Loss: 0.8773866891860962, G Loss: 1.1127054691314697\n",
      "Epoch: 21000/100000, Step: 1642000, D Loss: 0.7930381298065186, G Loss: 1.456311583518982\n",
      "Epoch: 21500/100000, Step: 1643000, D Loss: 0.6634434461593628, G Loss: 1.5527684688568115\n",
      "Epoch: 22000/100000, Step: 1644000, D Loss: 0.8917700052261353, G Loss: 1.3099110126495361\n",
      "Epoch: 22500/100000, Step: 1645000, D Loss: 1.0299930572509766, G Loss: 1.0623130798339844\n",
      "Epoch: 23000/100000, Step: 1646000, D Loss: 0.8524194955825806, G Loss: 1.1055660247802734\n",
      "Epoch: 23500/100000, Step: 1647000, D Loss: 0.721089243888855, G Loss: 1.0891907215118408\n",
      "Epoch: 24000/100000, Step: 1648000, D Loss: 0.7863017916679382, G Loss: 1.264402151107788\n",
      "Epoch: 24500/100000, Step: 1649000, D Loss: 0.8987960815429688, G Loss: 1.3329665660858154\n",
      "Epoch: 25000/100000, Step: 1650000, D Loss: 1.0837452411651611, G Loss: 0.9778918027877808\n",
      "Epoch: 25500/100000, Step: 1651000, D Loss: 0.7492551803588867, G Loss: 1.1066420078277588\n",
      "Epoch: 26000/100000, Step: 1652000, D Loss: 1.044423222541809, G Loss: 0.9575101137161255\n",
      "Epoch: 26500/100000, Step: 1653000, D Loss: 0.9956264495849609, G Loss: 1.2403730154037476\n",
      "Epoch: 27000/100000, Step: 1654000, D Loss: 0.791547417640686, G Loss: 1.0538142919540405\n",
      "Epoch: 27500/100000, Step: 1655000, D Loss: 1.0756597518920898, G Loss: 1.0048353672027588\n",
      "Epoch: 28000/100000, Step: 1656000, D Loss: 1.239242434501648, G Loss: 1.0248699188232422\n",
      "Epoch: 28500/100000, Step: 1657000, D Loss: 1.1276252269744873, G Loss: 1.2238426208496094\n",
      "Epoch: 29000/100000, Step: 1658000, D Loss: 1.0702574253082275, G Loss: 1.4546310901641846\n",
      "Epoch: 29500/100000, Step: 1659000, D Loss: 0.8114733695983887, G Loss: 1.1652672290802002\n",
      "Epoch: 30000/100000, Step: 1660000, D Loss: 0.8656657338142395, G Loss: 0.9175512194633484\n",
      "Epoch: 30500/100000, Step: 1661000, D Loss: 1.0356168746948242, G Loss: 1.088437795639038\n",
      "Epoch: 31000/100000, Step: 1662000, D Loss: 0.8391834497451782, G Loss: 1.2732501029968262\n",
      "Epoch: 31500/100000, Step: 1663000, D Loss: 0.8029397130012512, G Loss: 1.1372407674789429\n",
      "Epoch: 32000/100000, Step: 1664000, D Loss: 0.8482522368431091, G Loss: 1.082279920578003\n",
      "Epoch: 32500/100000, Step: 1665000, D Loss: 0.788227915763855, G Loss: 1.5635837316513062\n",
      "Epoch: 33000/100000, Step: 1666000, D Loss: 1.2936735153198242, G Loss: 1.259704828262329\n",
      "Epoch: 33500/100000, Step: 1667000, D Loss: 0.8713105916976929, G Loss: 1.4694888591766357\n",
      "Epoch: 34000/100000, Step: 1668000, D Loss: 0.8691803812980652, G Loss: 0.9110224843025208\n",
      "Epoch: 34500/100000, Step: 1669000, D Loss: 0.9276639223098755, G Loss: 1.1400988101959229\n",
      "Epoch: 35000/100000, Step: 1670000, D Loss: 0.7869230508804321, G Loss: 1.1966091394424438\n",
      "Epoch: 35500/100000, Step: 1671000, D Loss: 0.9887558221817017, G Loss: 1.0917922258377075\n",
      "Epoch: 36000/100000, Step: 1672000, D Loss: 0.7978925704956055, G Loss: 1.1579887866973877\n",
      "Epoch: 36500/100000, Step: 1673000, D Loss: 0.8301019668579102, G Loss: 1.130247712135315\n",
      "Epoch: 37000/100000, Step: 1674000, D Loss: 1.0765591859817505, G Loss: 1.1130223274230957\n",
      "Epoch: 37500/100000, Step: 1675000, D Loss: 0.7522369027137756, G Loss: 1.1965715885162354\n",
      "Epoch: 38000/100000, Step: 1676000, D Loss: 0.6689830422401428, G Loss: 1.171379804611206\n",
      "Epoch: 38500/100000, Step: 1677000, D Loss: 1.0299439430236816, G Loss: 1.0705020427703857\n",
      "Epoch: 39000/100000, Step: 1678000, D Loss: 1.1127021312713623, G Loss: 1.1045787334442139\n",
      "Epoch: 39500/100000, Step: 1679000, D Loss: 0.8964150547981262, G Loss: 1.1805696487426758\n",
      "Epoch: 40000/100000, Step: 1680000, D Loss: 0.8594578504562378, G Loss: 1.120730996131897\n",
      "Epoch: 40500/100000, Step: 1681000, D Loss: 0.7867957353591919, G Loss: 1.1700456142425537\n",
      "Epoch: 41000/100000, Step: 1682000, D Loss: 1.0316436290740967, G Loss: 1.083998441696167\n",
      "Epoch: 41500/100000, Step: 1683000, D Loss: 1.068138837814331, G Loss: 1.167551875114441\n",
      "Epoch: 42000/100000, Step: 1684000, D Loss: 0.729833722114563, G Loss: 1.2189522981643677\n",
      "Epoch: 42500/100000, Step: 1685000, D Loss: 0.6936858892440796, G Loss: 1.2222011089324951\n",
      "Epoch: 43000/100000, Step: 1686000, D Loss: 0.8278425931930542, G Loss: 1.2159199714660645\n",
      "Epoch: 43500/100000, Step: 1687000, D Loss: 0.9777089357376099, G Loss: 1.0823522806167603\n",
      "Epoch: 44000/100000, Step: 1688000, D Loss: 0.7777696847915649, G Loss: 1.259751319885254\n",
      "Epoch: 44500/100000, Step: 1689000, D Loss: 0.9444411396980286, G Loss: 1.1792447566986084\n",
      "Epoch: 45000/100000, Step: 1690000, D Loss: 1.007734775543213, G Loss: 1.2725677490234375\n",
      "Epoch: 45500/100000, Step: 1691000, D Loss: 0.8485940098762512, G Loss: 1.1887714862823486\n",
      "Epoch: 46000/100000, Step: 1692000, D Loss: 0.3757486939430237, G Loss: 1.0215460062026978\n",
      "Epoch: 46500/100000, Step: 1693000, D Loss: 1.0607186555862427, G Loss: 1.0649513006210327\n",
      "Epoch: 47000/100000, Step: 1694000, D Loss: 1.272072672843933, G Loss: 1.1162216663360596\n",
      "Epoch: 47500/100000, Step: 1695000, D Loss: 0.9203901886940002, G Loss: 1.028714895248413\n",
      "Epoch: 48000/100000, Step: 1696000, D Loss: 0.6814805269241333, G Loss: 1.2508227825164795\n",
      "Epoch: 48500/100000, Step: 1697000, D Loss: 1.2430363893508911, G Loss: 1.2075693607330322\n",
      "Epoch: 49000/100000, Step: 1698000, D Loss: 1.3069123029708862, G Loss: 1.1407641172409058\n",
      "Epoch: 49500/100000, Step: 1699000, D Loss: 0.9859679937362671, G Loss: 0.9530792236328125\n",
      "Epoch: 50000/100000, Step: 1700000, D Loss: 0.9302036762237549, G Loss: 1.0870606899261475\n",
      "Epoch: 50500/100000, Step: 1701000, D Loss: 1.0963441133499146, G Loss: 1.262319564819336\n",
      "Epoch: 51000/100000, Step: 1702000, D Loss: 1.049757957458496, G Loss: 1.0790456533432007\n",
      "Epoch: 51500/100000, Step: 1703000, D Loss: 0.83433997631073, G Loss: 1.1319233179092407\n",
      "Epoch: 52000/100000, Step: 1704000, D Loss: 1.146385908126831, G Loss: 1.1513320207595825\n",
      "Epoch: 52500/100000, Step: 1705000, D Loss: 0.7237210869789124, G Loss: 1.1849313974380493\n",
      "Epoch: 53000/100000, Step: 1706000, D Loss: 1.0379314422607422, G Loss: 1.1505110263824463\n",
      "Epoch: 53500/100000, Step: 1707000, D Loss: 0.7525656223297119, G Loss: 1.0750505924224854\n",
      "Epoch: 54000/100000, Step: 1708000, D Loss: 1.0457969903945923, G Loss: 1.2411320209503174\n",
      "Epoch: 54500/100000, Step: 1709000, D Loss: 1.0425090789794922, G Loss: 1.2391493320465088\n",
      "Epoch: 55000/100000, Step: 1710000, D Loss: 0.8978208303451538, G Loss: 1.2422001361846924\n",
      "Epoch: 55500/100000, Step: 1711000, D Loss: 0.6810692548751831, G Loss: 1.1598612070083618\n",
      "Epoch: 56000/100000, Step: 1712000, D Loss: 0.7889721393585205, G Loss: 1.1731655597686768\n",
      "Epoch: 56500/100000, Step: 1713000, D Loss: 0.7881922721862793, G Loss: 1.230616807937622\n",
      "Epoch: 57000/100000, Step: 1714000, D Loss: 0.7242478132247925, G Loss: 1.2714046239852905\n",
      "Epoch: 57500/100000, Step: 1715000, D Loss: 1.0509310960769653, G Loss: 1.0572205781936646\n",
      "Epoch: 58000/100000, Step: 1716000, D Loss: 0.8590472936630249, G Loss: 1.1653953790664673\n",
      "Epoch: 58500/100000, Step: 1717000, D Loss: 1.1286907196044922, G Loss: 1.1248233318328857\n",
      "Epoch: 59000/100000, Step: 1718000, D Loss: 0.8224270343780518, G Loss: 1.0917913913726807\n",
      "Epoch: 59500/100000, Step: 1719000, D Loss: 0.8158726692199707, G Loss: 1.3849703073501587\n",
      "Epoch: 60000/100000, Step: 1720000, D Loss: 0.7905021905899048, G Loss: 1.0982301235198975\n",
      "Epoch: 60500/100000, Step: 1721000, D Loss: 1.0367422103881836, G Loss: 1.130041241645813\n",
      "Epoch: 61000/100000, Step: 1722000, D Loss: 0.8580989241600037, G Loss: 1.0945641994476318\n",
      "Epoch: 61500/100000, Step: 1723000, D Loss: 0.849858283996582, G Loss: 1.3727326393127441\n",
      "Epoch: 62000/100000, Step: 1724000, D Loss: 1.1803721189498901, G Loss: 1.3604891300201416\n",
      "Epoch: 62500/100000, Step: 1725000, D Loss: 1.1041147708892822, G Loss: 1.171019434928894\n",
      "Epoch: 63000/100000, Step: 1726000, D Loss: 0.9781467318534851, G Loss: 1.3089946508407593\n",
      "Epoch: 63500/100000, Step: 1727000, D Loss: 1.1320024728775024, G Loss: 1.51298987865448\n",
      "Epoch: 64000/100000, Step: 1728000, D Loss: 0.8128143548965454, G Loss: 1.169144868850708\n",
      "Epoch: 64500/100000, Step: 1729000, D Loss: 0.795151948928833, G Loss: 1.178855538368225\n",
      "Epoch: 65000/100000, Step: 1730000, D Loss: 0.8128519058227539, G Loss: 1.0784927606582642\n",
      "Epoch: 65500/100000, Step: 1731000, D Loss: 1.0091010332107544, G Loss: 1.198922038078308\n",
      "Epoch: 66000/100000, Step: 1732000, D Loss: 0.846014142036438, G Loss: 0.9800348281860352\n",
      "Epoch: 66500/100000, Step: 1733000, D Loss: 1.038891077041626, G Loss: 1.1914737224578857\n",
      "Epoch: 67000/100000, Step: 1734000, D Loss: 0.8055380582809448, G Loss: 1.2505677938461304\n",
      "Epoch: 67500/100000, Step: 1735000, D Loss: 1.0619961023330688, G Loss: 1.176552653312683\n",
      "Epoch: 68000/100000, Step: 1736000, D Loss: 1.2685546875, G Loss: 1.2058608531951904\n",
      "Epoch: 68500/100000, Step: 1737000, D Loss: 0.8005809187889099, G Loss: 1.0147981643676758\n",
      "Epoch: 69000/100000, Step: 1738000, D Loss: 1.2650275230407715, G Loss: 1.1211209297180176\n",
      "Epoch: 69500/100000, Step: 1739000, D Loss: 0.7638579607009888, G Loss: 1.0688930749893188\n",
      "Epoch: 70000/100000, Step: 1740000, D Loss: 0.944503903388977, G Loss: 1.1458600759506226\n",
      "Epoch: 70500/100000, Step: 1741000, D Loss: 0.7572891712188721, G Loss: 1.1478091478347778\n",
      "Epoch: 71000/100000, Step: 1742000, D Loss: 1.0534822940826416, G Loss: 1.078064203262329\n",
      "Epoch: 71500/100000, Step: 1743000, D Loss: 1.1427394151687622, G Loss: 1.072545051574707\n",
      "Epoch: 72000/100000, Step: 1744000, D Loss: 0.7943847179412842, G Loss: 1.100865364074707\n",
      "Epoch: 72500/100000, Step: 1745000, D Loss: 0.8841671943664551, G Loss: 1.061400294303894\n",
      "Epoch: 73000/100000, Step: 1746000, D Loss: 1.07505202293396, G Loss: 1.127835988998413\n",
      "Epoch: 73500/100000, Step: 1747000, D Loss: 0.782907247543335, G Loss: 1.346186637878418\n",
      "Epoch: 74000/100000, Step: 1748000, D Loss: 0.8689360618591309, G Loss: 0.9741586446762085\n",
      "Epoch: 74500/100000, Step: 1749000, D Loss: 1.2781946659088135, G Loss: 1.5019289255142212\n",
      "Epoch: 75000/100000, Step: 1750000, D Loss: 0.7938075065612793, G Loss: 1.2007369995117188\n",
      "Epoch: 75500/100000, Step: 1751000, D Loss: 0.8961472511291504, G Loss: 1.1629207134246826\n",
      "Epoch: 76000/100000, Step: 1752000, D Loss: 1.0707988739013672, G Loss: 1.0517563819885254\n",
      "Epoch: 76500/100000, Step: 1753000, D Loss: 1.4077222347259521, G Loss: 1.1963186264038086\n",
      "Epoch: 77000/100000, Step: 1754000, D Loss: 1.0917305946350098, G Loss: 0.9921838045120239\n",
      "Epoch: 77500/100000, Step: 1755000, D Loss: 1.0156171321868896, G Loss: 1.146693229675293\n",
      "Epoch: 78000/100000, Step: 1756000, D Loss: 0.9996157288551331, G Loss: 1.0613654851913452\n",
      "Epoch: 78500/100000, Step: 1757000, D Loss: 0.8671427965164185, G Loss: 1.275942325592041\n",
      "Epoch: 79000/100000, Step: 1758000, D Loss: 0.5693023204803467, G Loss: 1.2800477743148804\n",
      "Epoch: 79500/100000, Step: 1759000, D Loss: 0.8702541589736938, G Loss: 1.1812913417816162\n",
      "Epoch: 80000/100000, Step: 1760000, D Loss: 1.026332974433899, G Loss: 1.3105039596557617\n",
      "Epoch: 80500/100000, Step: 1761000, D Loss: 0.7822826504707336, G Loss: 1.3465468883514404\n",
      "Epoch: 81000/100000, Step: 1762000, D Loss: 0.8395825624465942, G Loss: 1.1470553874969482\n",
      "Epoch: 81500/100000, Step: 1763000, D Loss: 0.8529956340789795, G Loss: 1.474090576171875\n",
      "Epoch: 82000/100000, Step: 1764000, D Loss: 1.2352657318115234, G Loss: 1.5891258716583252\n",
      "Epoch: 82500/100000, Step: 1765000, D Loss: 0.7370160818099976, G Loss: 1.109463095664978\n",
      "Epoch: 83000/100000, Step: 1766000, D Loss: 0.6306519508361816, G Loss: 1.251546025276184\n",
      "Epoch: 83500/100000, Step: 1767000, D Loss: 0.8293378353118896, G Loss: 1.2898356914520264\n",
      "Epoch: 84000/100000, Step: 1768000, D Loss: 0.9965150356292725, G Loss: 1.5799586772918701\n",
      "Epoch: 84500/100000, Step: 1769000, D Loss: 0.8684951066970825, G Loss: 1.2661106586456299\n",
      "Epoch: 85000/100000, Step: 1770000, D Loss: 0.830487847328186, G Loss: 1.3665002584457397\n",
      "Epoch: 85500/100000, Step: 1771000, D Loss: 0.548704206943512, G Loss: 1.2109122276306152\n",
      "Epoch: 86000/100000, Step: 1772000, D Loss: 0.8547915816307068, G Loss: 1.2200666666030884\n",
      "Epoch: 86500/100000, Step: 1773000, D Loss: 0.980090856552124, G Loss: 1.1361563205718994\n",
      "Epoch: 87000/100000, Step: 1774000, D Loss: 0.7484414577484131, G Loss: 1.227881669998169\n",
      "Epoch: 87500/100000, Step: 1775000, D Loss: 0.6001293659210205, G Loss: 1.1075992584228516\n",
      "Epoch: 88000/100000, Step: 1776000, D Loss: 0.7941250205039978, G Loss: 1.1372472047805786\n",
      "Epoch: 88500/100000, Step: 1777000, D Loss: 0.7894631624221802, G Loss: 1.5934966802597046\n",
      "Epoch: 89000/100000, Step: 1778000, D Loss: 0.5196095108985901, G Loss: 1.302439570426941\n",
      "Epoch: 89500/100000, Step: 1779000, D Loss: 0.7887015342712402, G Loss: 1.270262360572815\n",
      "Epoch: 90000/100000, Step: 1780000, D Loss: 0.5048903822898865, G Loss: 1.089132308959961\n",
      "Epoch: 90500/100000, Step: 1781000, D Loss: 0.7952816486358643, G Loss: 1.1250263452529907\n",
      "Epoch: 91000/100000, Step: 1782000, D Loss: 0.8477867841720581, G Loss: 1.2201639413833618\n",
      "Epoch: 91500/100000, Step: 1783000, D Loss: 0.8386249542236328, G Loss: 1.1607969999313354\n",
      "Epoch: 92000/100000, Step: 1784000, D Loss: 1.059680700302124, G Loss: 1.0836703777313232\n",
      "Epoch: 92500/100000, Step: 1785000, D Loss: 1.192920446395874, G Loss: 1.47747802734375\n",
      "Epoch: 93000/100000, Step: 1786000, D Loss: 0.9444074630737305, G Loss: 1.5841128826141357\n",
      "Epoch: 93500/100000, Step: 1787000, D Loss: 0.8303577899932861, G Loss: 1.1080610752105713\n",
      "Epoch: 94000/100000, Step: 1788000, D Loss: 0.8813478946685791, G Loss: 1.2394574880599976\n",
      "Epoch: 94500/100000, Step: 1789000, D Loss: 1.2876607179641724, G Loss: 1.2489649057388306\n",
      "Epoch: 95000/100000, Step: 1790000, D Loss: 0.6808409094810486, G Loss: 1.2472327947616577\n",
      "Epoch: 95500/100000, Step: 1791000, D Loss: 0.8279309272766113, G Loss: 1.0923999547958374\n",
      "Epoch: 96000/100000, Step: 1792000, D Loss: 1.073241949081421, G Loss: 1.1363357305526733\n",
      "Epoch: 96500/100000, Step: 1793000, D Loss: 0.7014207243919373, G Loss: 1.1389234066009521\n",
      "Epoch: 97000/100000, Step: 1794000, D Loss: 0.7695877552032471, G Loss: 1.1881067752838135\n",
      "Epoch: 97500/100000, Step: 1795000, D Loss: 0.9988081455230713, G Loss: 1.404005527496338\n",
      "Epoch: 98000/100000, Step: 1796000, D Loss: 1.2927992343902588, G Loss: 1.0255012512207031\n",
      "Epoch: 98500/100000, Step: 1797000, D Loss: 0.7278378009796143, G Loss: 1.079237699508667\n",
      "Epoch: 99000/100000, Step: 1798000, D Loss: 0.9802983999252319, G Loss: 1.1638383865356445\n",
      "Epoch: 99500/100000, Step: 1799000, D Loss: 0.8356455564498901, G Loss: 1.1192655563354492\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    for idx, (images, _) in enumerate(load_dataset()):\n",
    "        # Training Discriminator\n",
    "        x = images.to(DEVICE)\n",
    "        \n",
    "        x_outputs = D(x)\n",
    "        # print('x_outputs')\n",
    "        # print(x_outputs)\n",
    "        D_x_loss = criterion(x_outputs, D_labels)\n",
    "\n",
    "        z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
    "        z_outputs = D(G(z))\n",
    "        D_z_loss = criterion(z_outputs, D_fakes)\n",
    "        D_loss = D_x_loss + D_z_loss\n",
    "        \n",
    "        D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_opt.step()\n",
    "\n",
    "        if step % n_critic == 0:\n",
    "            # Training Generator\n",
    "            z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
    "            z_outputs = D(G(z))\n",
    "            G_loss = criterion(z_outputs, D_labels)\n",
    "\n",
    "            G.zero_grad()\n",
    "            G_loss.backward()\n",
    "            G_opt.step()\n",
    "            \n",
    "        if step % 1000 == 0:\n",
    "            print('Epoch: {}/{}, Step: {}, D Loss: {}, G Loss: {}'.format(epoch, max_epoch, step, D_loss.item(), G_loss.item()))\n",
    "        \n",
    "        if step % 2000 == 0:\n",
    "            save_checkpoint({'global_step': step,\n",
    "                'D':D.state_dict(),\n",
    "                'G':G.state_dict(),\n",
    "                'd_optim': D_opt.state_dict(),\n",
    "                'g_optim' : G_opt.state_dict()},\n",
    "                'ckpt/{}_{:07d}.pth.tar'.format(MODEL_NAME, step))\n",
    "            G.eval()\n",
    "            images = get_sample_image(G, n_noise)\n",
    "            eps_save_series(step, images)\n",
    "            # imsave('samples/{}_step{}.jpg'.format(MODEL_NAME, str(step).zfill(3)), img, cmap='gray')\n",
    "            G.train()\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint({'global_step': step,\n",
    "                'D':D.state_dict(),\n",
    "                'G':G.state_dict(),\n",
    "                'd_optim': D_opt.state_dict(),\n",
    "                'g_optim' : G_opt.state_dict()},\n",
    "                'ckpt/{}_{:06d}.pth.tar'.format(MODEL_NAME, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import glob\n",
    "# G_path = sorted(glob.glob(os.path.join('ckpt', 'began080000-good.pth.tar')))[0]\n",
    "# print(G_path)\n",
    "\n",
    "# /home/tc/ws/geo-to-picture/ckpt/VanillaGAN_1398000.pth.tar\n",
    "\n",
    "state = torch.load('ckpt/VanillaGAN_1398000.pth.tar')\n",
    "# print(state['G'])\n",
    "G.load_state_dict(state['G'])\n",
    "D.load_state_dict(state['D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = images.to(DEVICE)\n",
    "n_images = normalize_to_image(x)\n",
    "eps_save_series(3000, n_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 500, 4]' is invalid for input of size 10000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-503-697df12c1038>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# plt.figure(figsize = (15,15))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# plt.imshow(get_sample_image(G, n_noise), cmap='gray')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 500, 4]' is invalid for input of size 10000"
     ]
    }
   ],
   "source": [
    "G.eval()\n",
    "# plt.figure(figsize = (15,15))\n",
    "# plt.imshow(get_sample_image(G, n_noise), cmap='gray')\n",
    "G(z).view(2, eps_rows, image_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(D.state_dict(), 'D.pkl')\n",
    "# torch.save(G.state_dict(), 'G.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
